# 6장. 키-값 저장소 설계

- 키-값 저장소(key-value store) : 비 관계형 데이터 데이터베이스이고 고유한 식별자를 키로 가져야함

- 키-값 쌍에서의 키는 유일해야하며 값은 키를 통해서만 접근할 수 있음

- 키는 일반 텍스트 혹은 해시 값이고, 성능상의 이유로 키는 짧을 수록 좋음

- 값은 문자열, 리스트, 객체 등 어떤 값이 오든 상관하지 않음

- 아마존 다이나모, memcached, 레디스 등이 있음

  

## 문제 이해 및 설계 범위 확정

- 읽기, 쓰기 그리고 메모리 사용량 사이에 어떤 균형을 찾고, 데이터의 일관성과 가용성 사이에서 타협적 결정을 내려 설계함
- 설계 사항 예시
  - 키-값 쌍의 크기는 10KB
  - 큰 데이터를 저장할 수 있음
  - 높은 가용성 제공(시스템은 장애가 있어도 빠른 응답)
  - 높은 규모의 확장성을 제공(트래픽 양에 따라 자동적으로 서버 증설/삭제가 이뤄져야함)
  - 데이터 일관성 수준은 조정이 가능해야함
  - 짧은 응답 지연시간



## 단일 서버 키-값 저장소

- 키-값 쌍 전부를 메모리에 해시 테이블로 저장
  - 직관적이고 빠른 속도를 보장
  - 모든 데이터를 메모리 안에 두는 것이 불가능할 수 있는 약점을 가짐
  - 개선책으로 데이터 압축 혹은 자주 쓰이는 데이터만 메모리에 두고 나머지는 디스크에 저장하는 방법을 쓸 수 있으나 한 대 서버로 부족한 상황이 발생할 수 있음



## 분산 키-값 저장소

- 분산 키-값 저장소 = 분산 해시 테이블 : 키-값 쌍을 여러 서버에 분산시킴



### CAP 정리(Consistency, Availability, Partition Tolerance theorem)

- 일관성, 가용성, 파티션 감내의 세 가지 요구사항을 동시에 만족하는 분산 시스템을 설계하는 것은 불가능하다는 정리
- 데이터 일관성(Consistency) : 분산 시스템에 접속하는 모든 클라이언트는 어떤 노드에 접속했는지와 관계없이 언제나 같은 데이터를 보게 되어야 함
- 가용성(Availability) : 분산 시스템에 접속하는 클라이언트는 일부 노드에 장애가 발생하더라도 항상 응답을 받을 수 있어야함
- 파티션 감내(Partition Tolerance) : 파티션은 두 노드 사이에 통신 장애가 발생하였음을 의미함. 파티션 감내는 네트워크에 파티션이 생기더라도 시스템은 계속 동작하여야 함

![Untitled](https://user-images.githubusercontent.com/90545926/183422520-3825d987-5dcf-4b4c-bd21-339737db405c.png)

- CP : 가용성을 희생

- AP : 데이터 일관성을 희생

- CA : 파티션 감내를 희생. 그러나 통상 네트워크 장애는 피할 수 없기 때문에 분산 시스템은 반드시 파티션 문제를 감내할 수 있도록 설계되야함

  

<aside> 💡 CAP 첨언

[CAP Theorem, 오해와 진실](http://eincs.com/2013/07/misleading-and-truth-of-cap-theorem/)

- C와 A는 분산시스템의 특성이지만, P는 그 분산시스템이 돌아가는 네트워크에 대한 특성

- P의 정의는 네트워크가 임의의 메시지 손실을 허용할 수 있는지 여부. 다시 말해 네트워크가 가끔 장애가 날 수 있다는 것을 인정하느냐는 것.

- P의 선택을 배제하려면, **절대로 장애가 나지 않는 네트워크를 구성해야 하지만 그런 것은 이 세상에 존재하지 않음.** 따라서 **원하든 원하지 않든 P는 선택되어야 하며 결국 선택권은 C나 A중 하나** 밖에 없음

- 결국, CAP Theorem이 내포하고 있는 의미는 **분산시스템에서 네트워크 장애상황일 때 일관성과 가용성 중 많아도 하나만 선택할 수 있다**는 것

- CAP는 분산시스템 디자이너의 선택에 도움을 주는 정리이고, 정상상황일 때와 장애상황을 때를 나누어 설명하자는 것이 PACELC

- PACELC : **파티션(네트워크 장애)상황일 때에는 A와 C는 상충하며 둘 중 하나를 선택, 정상상황일 때에는 L과 C는 상충하며 둘 중 하나를 선택** 

  </aside>

- 이상적 상태 : 네트워크가 파티션되는 상황은 절대로 일어나지 않을 것이 않고, n1에 기록된 데이터는 자동적으로 n2, n3에 복제되어 데이터 일관성과 가용성도 만족됨
- 실세계의 분산 시스템 : 파티션 문제가 발생하면 일관성과 가용성 사이에서 하나를 선택해야 함.
  - 가용성 대신 일관성을 선택(CP) : 장애가 일어나 데이터가 제대로 전달되지 않으면 데이터 불일치가 일어나 쓰기 연산을 중단시키면 가용성이 깨짐. 은행권 시스템은 보통 데이터 일관성을 양보하지 않음
  - 일관성 대신 가용성 선택(AP) : 낡은 데이터를 반환할 위험이 있더라도 계속 읽기 연산을 허용



### 시스템 컴포넌트

- 데이터 파티션

  - 대규모 애플리케이션의 데이터를 작은 파티션으로 분할한 다음 여러대 서버에 저장
  - 고려 사항 : 1. 데이터를 여러 서버에 고르게 분산할 수 있는가, 2. 노드가 추가되거나 삭제될 때 데이터의 이동을 최소화할 수 있는가
  - 안정 해시를 사용해 데이터를 파티션하면 좋은 점 :
    - 규모 확장 자동화(automatic scaling) : 시스템 부하에 따라 서버가 자동으로 추가되거나 삭제되도록 만들 수 있음
    - 다양성(heterogeneity) : 각 서버의 용량에 맞게 가상노드의 수를 조정할 수 있음.

- 데이터 다중화(replication)

  - 높은 가용성과 안정성을 확보하기 위해서 데이터를 N개 서버에 비동기적으로 다중화할 필요
  - N개  서버를 선정하는 방법: 어떤 키를 해시링 위에 배치한 후, 시계 방향으로 링을 순회하면서 만나는 첫 N개 서버에 데이터 사본을 보관
  - 가상 노드를 사용하면 선택한 N개의 노드가 대응될 실제 물리서버의 개수가 N보다 작아질 수 있음 → 같은 물리서버를 중복선택하지 않도록 함
  - 안정성을 담보하기 위해 데이터 사본은 다른 센터에 보관하고 센터들은 고속 네트워크로 연결

  ![Untitled](https://user-images.githubusercontent.com/90545926/183422493-d41e8369-15d5-4e0b-853f-c4df266e00b3.png)

- 일관성(consistency)

  - 정족수 합의(Quorum Consensus) 프로토콜을 사용하면 읽기/쓰기 연산 모두에 일관성을 보장할 수 있음

    - N = 사본 개수

    - W = 쓰기 연산에 대한 정족수. 쓰기 연산이 성공한 것으로 간주되려면 적어도 W개의 서버로부터 쓰기 연산이 성공했다는 응답을 받아야함

    - R = 읽기 연산에 대한 정족수. 읽기 연산이 성공한 것으로 간주되려면 적어도 R개의 서버로부터 응답을 받아야함

      ![N=3인 경우](https://user-images.githubusercontent.com/90545926/183422504-13e654ba-a0b3-490b-888d-63e129033cb1.png)

      N=3인 경우

    - w=1의 의미는 중재자가 쓰기 연산이 성공했다고 판단하기 위해 최소 한 대의 서버로부터 쓰기 성공 응답을 받아야 한다는 뜻

    - 중재자는 클라이언트와 노드사이의 프락시(proxy) 역할을 함

    - W,R,N값을 정하는 것은 응답 지연과 데이터 일관성 사이의 타협점을 찾는 과정

    - W+R>N인 경우 일관성을 보증할 최신 데이터를 가진 노드가 최소 하나를 겹치기 때문에 강한 일관성이 보장됨

      - R = 1, W = N : 빠른 읽기 연산에 최적화된 시스템
      - W = 1, R = N : 빠른 쓰기 연산에 최적화된 시스템
      - W + R > N : 강한 일관성이 보장됨(보통 N=3, W=R=2)
      - W + R <= N : 강한 일관성이 보장되지 않음

  - 일관성 모델 : 일관성의 수준을 결정

    - 강한 일관성 :
      - 모든 읽기 연산은 가장 최근에 갱신된 결과를 반환(클라이언트는 낡은 데이터를 절대로 보지 못함)
      - 모든 사본에 현재 쓰기 연산의 결과가 반영될 떄까지 해당 데이터에 대한 읽기/쓰기를 금지
      - 새로운 요청의 처리가 중단되기 때문에 고가용성의 시스템에는 적합하지 않음
    - 약한 일관성 : 읽기 연산은 가장 최근에 갱신된 결과를 반환하지 못할 수도 있음
    - 최종 일관성 :
      - 약한 일관성의 한 형태로, 갱신 결과가 결국에는 모든 사본에 반영되는 모델
      - 다이나모 또는 카산드라 같은 저장소가 선택하는 모델
      - 최종 일관성 모델로 쓰기 연산이 병렬적으로 발생해 시스템에 저장된 값의 일관성이 깨어지는 문제는 클라이언트가 해결해야함

  - 비 일관성 해소기법 : 데이터 버저닝

    - 버저닝(versioning)과 백터시계(vector clock)는 데이터 다중화 일관성 문제를 해소하기 위해 등장한 기술
    - 버저닝은 데이터를 변경할때마다 해당 데이터의 새로운 버전을 만드는 것. 각 버전의 데이터는 변경 불가능
    - 백터시계는 [서버, 버전]의 순서쌍을 데이터에 매단 것. 버전의 선후행과 충돌여부를 판별하는데 사용

    ![Untitled](https://user-images.githubusercontent.com/90545926/183422507-64e49822-2ba0-43ce-bb93-329081f82f12.png)

    - 벡터 시계 사용 단점 :
      - 충돌 감지 및 해소 로직이 클라이언트에 들어가야해 클라이언트 구현이 복잡해짐
      - [서버:버전]의 순서쌍 개수가 굉장히 빨리 늘어남 → 임계치를 설정하고 넘어가면 오래된 순서쌍을 제거하도록 해야함(버전간 선후 관계가 정확하지 않을 수 있어 효율성이 낮아 질 수 있음)

  - 장애처리

    - 장애감지 : 분산시스템에서 보통 두 대 이상의 서버가 똑같이 서버 A의 장애를 보고해야 해당 서버에 실제로 장애가 발생했다고 간주

      - 모든 노드사이에 멀티캐스팅 채널을 구축하는 것이 장애를 감지하는 가장 쉬운 방법이나 서버가 많으면 비효율 → 가십 프로토콜(gossip protocol) 같은 분산형 장애 감지 솔루션을 채택하는 편이 효율적
      - 가십 프로토콜
        - 각 노드는 멤버십 목록을 유지(각 멤버 ID와 그 박동카운터-heartbeat counter쌍의 목록)
        - 각 노드는 주기적으로 heartbeat count를 증가
        - 각 노드는 무작위로 선정된 노드에게 주기적으로 heartbeat count 목록을 보냄
        - 받은  노드는 멤버십 목록을 최신 값으로 갱신
        - 어떤 멤버의 heartbeat count값이 지정된 시간동안 갱신되지 않으면 해당 멤버는 장애 상태로 간주

      ![Untitled](https://user-images.githubusercontent.com/90545926/183422518-2a59854d-b14e-4639-8085-2cd0189d7eb1.png)

    - 일시적 장애처리

      - 엄격한 정족수 접근법 : 읽기와 쓰기 연산 금지
      - 느슨한 정족수 접근법(단서 후 임시위탁 기법) :
        - 조건을 완화해 가용성을 높임
        - 정족수 요구사항을 강제하는 대신 쓰기 연산을 수행할 W개의 건강한 서버와 읽기 연산을 수행할 R개의 건강한 서버를 해시링에서 고름
        - 네트워크나 서버문제로 장애 상태인 서버로 가는 요청은 다른 서버가 잠시 맡아 처리하고, 그동안 발생한 변경사항은 해당 서버가 복구되었을 때 일괄 반영해 데이터 일관성을 보존

    - 영구 장애처리

      - 반-엔트로피 프로토콜을 구현해 사본들을 동기화(사본들을 비교해 최신버전으로 갱신하는 과정으로 포함)

      - 사본간의 일관성이 망가진 상태를 탐지하고 전송 데이터의 양을 줄이기 위해서 머클(Merkle)트리를 사용

      - 머클트리 : 해시 트리라고도 불림. 각 노드에 그 자식 노드들에 보관된 값의 해시(자식 노드가 종단 노드인 경우) 또는 자식 노드들의 레이블로부터 계산된 해시값을 레이블로 붙여두는 트리

      - 해시 트리를 사용하면 대규모 자료 구조의 내용을 효과적이면서도 보안상 안전한 방법으로 검증할 수 있음

        

      <aside> 💡 머클트리 만드는 예제

      1. 키 공간을 다음의 그림과 같이 버킷으로 나눈다. (예제에서는 4개 버킷으로 나눴다.)

         ![https://daeakin.github.io//images/large-system/merkle-tree.png](https://daeakin.github.io//images/large-system/merkle-tree.png)

      2. 버킷에 포함된 각각의 키에 균등 분포 해시(uniform hash) 함수를 적용하여 해시 값을 계산한다.

         ![https://daeakin.github.io//images/large-system/merkle-tree-2.png](https://daeakin.github.io//images/large-system/merkle-tree-2.png)

      3. 버킷별로 해시값을 계산한 후, 해당 해시 값을 레이블로 갖는 노드를 만든다.

         ![https://daeakin.github.io//images/large-system/merkle-tree-3.png](https://daeakin.github.io//images/large-system/merkle-tree-3.png)

      4. 자식 노드의 레이블로부터 새로운 해시 값을 계산하여, 이진 트리를 상향식으로 구성해 나간다.

         ![https://daeakin.github.io//images/large-system/merkle-tree-4.png](https://daeakin.github.io//images/large-system/merkle-tree-4.png)

      </aside>

      

      - 두 머클 트리의 비교는 루트 노드의 해시값을 비교하는 것으로 시작함
      - 루트 노드의 해시 값이 일치한다면 두 서버는 같은 데이터를 갖는 것
      - 값이 다른 경우 외쪽 자식 노드의 해시값을 비교하고, 그 다음으로는 오른쪽 자식 노드의 해시값을 비교. 이렇게 탐색해나가 다른 데이터를 갖는 버킷을 찾고 그 버킷들만 동기화하면 됨
      - 머클 트리을 사용하면 동기화해야하는 데이터 양은 실제로 존재하는 크기에 비례할뿐 두 서버에 보관된 데이터의 총량과의 무관(하지만 실제로 쓰이는 시스템의 경우 버킷 하나의 크기가 꽤 큼)

    - 데이터 센터 장애처리

      - 데이터 센터 장애에 대응하기 위해서는 데이터를 여러 데이터 센터에 다중화하는 것이 중요

- 시스템 아키텍처 다이어그램

  - 클라이언트는 키-값 저장소가 제공하는 두가진 단순한 API, 즉 get(key) 및 put(key, value)와 통신
  - 중재자는 클라이언트에게 키-값 저장소에 대한 프락시 역할을 하는 노드
  - 노드는 안정해시의 해시링 위에 분포
  - 노드는 자동으로 추가 또는 삭제할 수 있도록 시스템은 완전히 분산됨
  - 데이터는 여러 노드에 다중화
  - 모든 노드가 같은 책임을 지므로 SPOF(single point of failure)는 존재하지 않음
  - 완전 분산된 설계를 채택함으로 클라이언트API, 장애감지, 데이터 충돌해소, 장애 복구 메커니즘, 다중화, 저장소 엔진 등을 전부 지원해야함



### 카산드라의 사례

- 쓰기 경로(write path)

  - 쓰기 요청이 커밋 로그 파일에 기록 → 데이터가 메모리 캐시에 기록 → 메모리 캐시가 가득차거나 사전에 정의된 어떤 임계치에 도달하면 데이터는 디스크에 있는 SSTable(Sorted-String Table, <키, 값>의 순서쌍을 정렬된 리스트 형태로 관리하는 테이블)에 기록

- 읽기 경로(read path)

  - 읽기 요청을 받은 노드는 데이터가 메모리 캐시에 있는지 확인 → 있으면 해당 데이터를 클라이언트에게 반환, 없으면 디스크에서 가져옴

  - 디스크에 있는 SSTable에서 데이터를 효율적으로 찾는 방법으로 흔히 블룸필터(Bloom filter)를 사용

    

    <aside> 💡 블룸 필터 : 해시 테이블을 활용해서 현재 url이 찾고자 하는 데이터 인지를 찾아내는 방법

    [블룸 필터 - 위키백과, 우리 모두의 백과사전](https://ko.wikipedia.org/wiki/블룸_필터)

    [BloomFilter를 이용해서 데이터 찾기.](https://yujuwon.tistory.com/entry/BloomFilter를-이용해서-데이터-찾기)

    </aside>

    

  - 데이터가 메모리에 있는지 검사 → 없으면 블룸 필터를 검사 → 블룸 필터를 통해 어떤 SSTable에 키가 보관되어 있는지 알아냄 → SSTable에서 데이터를 가져옴 → 해당 데이터를 클라이언트에게 반환



## 요약

| 목표/문제                         | 기술                                                         |
| --------------------------------- | ------------------------------------------------------------ |
| 대규모 데이터 저장                | 안정 해시를 사용해 서버들에 부하 분산                        |
| 읽기 연산에 대한 높은 가용성 보장 | 데이터를 여러 데이터센터에 다중화                            |
| 쓰기 연산에 대한 높은 가용성 보장 | 버저닝 및 벡터 시계를 사용한 충돌 해소                       |
| 데이터 파티션                     | 안정 해시                                                    |
| 점진적 규모 확장성                | 안정 해시                                                    |
| 다양성(heteroheneity)             | 안정 해시                                                    |
| 조절 가능한 데이터 일관성         | 정족수 합의(quorum concensus)                                |
| 일시적 장애 처리                  | 느슨한 정족수 프로토콜(sloppy quorum)과 단서 후 임시 위탁(hinted handoff) |
| 영구적 장애 처리                  | 머클 트리(Merkle tree)                                       |
| 데이터 센터 장애 대응             | 여러 데이터 센터에 걸친 데이터 다중화                        |