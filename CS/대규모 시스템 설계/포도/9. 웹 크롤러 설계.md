# 웹 크롤러 설계

크롤러의 용도

- 검색 엔진 인덱싱 : 크롤러의 가장 보편적인 용례로 웹 페이지를 모아 검색 엔진을 위한 로컬 인덱스를 만든다. Googlebot은 구글이 사용하는 웹 크롤러다.
- 웹 아카이빙 : 나중에 사용할 목적으로 장기보관하기 위해 웹에서 정보를 모으는 절차를 말한다. 
- 웹 마이닝 : 웹 마이닝을 통해 인터넷에서 유용한 지식을 도출할 수 있다.
- 웹 모니터링 : 인터넷에서 저작권이나 상표권이 침해되는 사례를 모니터링할 수 있다.

<br>

## 설계 범위 확정
웹 크롤러의 기본 알고리즘
1. URL 집합이 입력으로 주어지면 해당 URL들이 가리키는 모든 웹 페이지를 다운로드
2. 다운받은 웹 페이지에서 URL을 추출
3. 추출된 URL을 다운로드할 URL 목록에 추가한 후 1번부터 반복

<br>

주의해야 할 속성
- 규모 확장성 : 오늘날 웹에는 수십억 개의 페이지가 존재하기 때문에 병행성을 활용하면 더 효과적으로 웹 크롤링을 할 수 있다.
- 안정성 : 비정상적인 입력이나 환경에 잘 대응할 수 있어야 한다.
- 예절 : 수집 대상 웹 사이트에 짧은 시간 동안 너무 많은 요청을 보내서는 안 된다.
- 확장성 : 새로운 형태의 콘텐츠를 지원하기 쉬워야 한다. 

<br>

### 개략적 규모 추정
- 매달 10억개의 웹 페이지 다운로드
- QPS = 10억/30일/24시간/3600초 = 약 400페이지/초
- 최대 QPS = 2 × QPS = 800
- 웹 페이지의 크기 평균 : 500k
- 10억 페이지 × 500k = 500TB/월
- 5년간 필요한 저장 용량 : 500TB × 12개월 × 5년 = 30PB

** QPS(Queries-per-second) : 초당 처리할 수 있는 쿼리 수
