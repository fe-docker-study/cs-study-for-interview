# 안정 해시 설계
안정 해시는 ‘수평적 규모 확장성을 위해 데이터를 서버에 균등하게 나누는’ 기술이다. 

<img width="473" alt="image" src="https://user-images.githubusercontent.com/31172248/182128969-5bd09874-f36b-4825-8b2e-c2565263382b.png">


## Issue : 해시 키 재배치 문제
N개의 캐시 서버가 있을 때, 이 서버들에 부하를 균등하게 나누는 보편적 방법은 해시함수를 사용하는 것이다.
> serverIndex = hash(key) % N
<img width="472" alt="image" src="https://user-images.githubusercontent.com/31172248/182130472-3e5b4017-5ead-4e96-b1a2-9abf7a7e4959.png">
<img width="473" alt="image" src="https://user-images.githubusercontent.com/31172248/182130782-84cc0395-dced-4215-8266-2b8f8a87318f.png">

특정한 키가 보관된 서버를 알아내기 위해 나머지(modular) 연산을 f(key)%4로 적용했다. 만약, hash(key0)%4=1이면, 클라이언트는 캐시에 보관된 데이터를 가져오기 위해 서버1에 접속해야 한다.

하지만, 이 방법은 서버들의 크기가 고정되어있을 때, 데이터 분포가 균등할 때 잘 동작한다.
서버가 추가되거나, 삭제되면 문제가 생긴다.
만약 서버 풀의 크기가 3으로 줄면 다음과 같이 적용된 결과가 나온다.

<img width="473" alt="image" src="https://user-images.githubusercontent.com/31172248/182130914-84929d28-b8b2-4c2f-a382-3dc7ea472391.png">
<img width="473" alt="image" src="https://user-images.githubusercontent.com/31172248/182130989-f2b894a3-664c-4fd1-8597-3dfde04ec6eb.png">


다음은 변화 된 키 분포를 보여준다.
장애가 발생한 1번 서버에 보관되어 있는 키 뿐만 아닌 대부분의 키가 재분배 되었다. 1번 서버가 죽으면 대부분 캐시 클라이언트가 데이터가 없는 엉뚱한 서버에 접속하게 된다.(cache miss) 안정해시는 이 문제를 효과적으로 해결하는 기술이다.

## 안정해시
안정해시는 해시 테이블 크기가 조정될 때 평균적으로 오직 k/n개의 키만 재배치하는 해시 기술이다. 여기서 k는 키의 개수, n은 슬롯(slot)의 개수이다. 이와 달리 전통적 해시 테이블은 슬롯의 수가 바뀌면 거의 대부분 키를 재배치한다.

### 해시 공간과 해시 링
해시 함수 f는 SHA-1를 사용한다고 하고, 함수의 출력 값 범위는 x0,x1,x2,x3,…xn과 같다고 했을 때, SHA-1의 해시 공간 범위는 0부터 2^160-1까지라고 할 수 있다.

![sha_algorithm_type](https://user-images.githubusercontent.com/31172248/182131267-4745ee36-90d8-4763-b22c-28ddc660fe7f.png)

따라서 x0 = 0, xn = 2^160 -1 의 공간을 그림으로 나타내어 구부려 접으면 해시링이 만들어진다.

<img width="473" alt="image" src="https://user-images.githubusercontent.com/31172248/182131549-02263e33-7477-42b0-be35-852ecee8867a.png">
<img width="473" alt="image" src="https://user-images.githubusercontent.com/31172248/182131604-b5194135-db7e-4e48-a5f4-a1290aad6156.png">


### 해시 서버
<img width="473" alt="image" src="https://user-images.githubusercontent.com/31172248/182133386-299b4960-6e10-4450-97da-7ddff0151109.png">

이 해시 함수 f를 사용하면 서버 IP나 이름을 이 링 위의 어떤 위치에 대응시킬 수 있다. 
아래와 같이 4개의 서버를 링 위에 배치해보았다.

### 해시 키
<img width="473" alt="image" src="https://user-images.githubusercontent.com/31172248/182133461-7689baf1-96df-47d0-8ec5-3f887f089f14.png">

그리고 다음과 같이 캐시할 키 key0, key1, key2, key3를 링 위의 어떤 위치에 배치할 수 있다.

> 여기 사용된 해시 함수는 앞선 “해시 키 재배치 문제”에 사용된 함수와는 다르다. 

### 서버 조회
<img width="473" alt="image" src="https://user-images.githubusercontent.com/31172248/182131842-dbcf54a8-753b-41ec-8f96-f1006b19c15a.png">

**어떤 키가 저장된든 서버**는, 해당 키의 위치로부터 시계방향으로 링을 탐색하면서 만나는 첫 번째 서버이다. 

### 서버 추가

서버를 추가해야할 땐 어떻게 해야할까? 키 가운데 일부만 재배치하면 된다. 
다음 그림에서 `서버4`가 추가 되었을 때 key0만 재배치한 것을 볼 수 있다. 나머지 키들은 재배치 하지 않는다.

<img width="473" alt="image" src="https://user-images.githubusercontent.com/31172248/182131953-b4e4552d-684a-44ca-9372-90e65a239d93.png">


### 서버 제거
<img width="473" alt="image" src="https://user-images.githubusercontent.com/31172248/182132128-b9021d6d-e0c7-4582-80b5-a0295cf40beb.png">

하나의 서버가 제거되면 키의 일부만 재배치한다. 


## 기본 구현법의 두 가지 문제
안정해시 알고리즘의 절차는 다음과 같다.
* 서버와 키를 **균등 분포(uniform distribution) 해시 함수**를 사용해 해시 링에 배치하는 것.
* 키의 위치에서 링을 시계 방향으로 탐색하다가 만나는 최초의 서버가 키가 저장되는 서버다.

다음 접근 법에는 두 가지 문제가 있는데 `서버가 추가되거나 삭제되는 상황을 감안하면 파티션의 크기를 균등하게 유지하는게 불가능하다` 여기서 파티션은 **인접한 서버 사이의 해시 공간**이다. 만약 어떤 서버는 작은 해시 공간을 할당받고, 어떤 서버는 큰 해시 공간을 할당받게 되어버리는 것이다. 

<img width="473" alt="image" src="https://user-images.githubusercontent.com/31172248/182132179-661355ec-afaf-40c1-af92-a36e2a3c18c6.png">

두 번째 문제는 `키의 균등 분포를 달성하기 어렵다`는 것이다. 예를들어, 서버가 그림과 같이 있을 때, 서버1과 서버3은 아무 데이터도 저장되지 않고 대 부분의 키가 서버2에 몰릴 것이다.

<img width="473" alt="image" src="https://user-images.githubusercontent.com/31172248/182132256-f5e56d9d-7a61-461a-aa7f-137eb29c8a1a.png">


## 가상노드
위 문제를 해결하기 위해 제안된 기법이 **가상 노드**기법이다.
가상 노드는 **실제 노드 또는 서버를 가리키는 노드**이다. 하나의 서버는 링 위에 여러 개의 가상 노드를 가질 수 있다. 
아래 그림에서 서버1은 임의의 개수인 서버 3개를 가진다. 

<img width="473" alt="image" src="https://user-images.githubusercontent.com/31172248/182132340-5d157ea3-c9b1-47e6-b537-20e13982b8cc.png">

서버 0을 링에 배치하기 우해 s0_0, s0_1, s0_2 세 개의 가상 노드를 사용하였고, 서버 1을 링에 배치할 때는 s1_0, s1_1, s1_2의 세 개 가상 노드를 사용했다.
이 각 서버는 하나가 아닌 여러 개 파티션을 관리하면 된다. 
키의 위치로부터 시계방향으로 링을 탐색하다 만나는 최초의 가상 노드가 해당 키가 저장될 서버가 된다. 
k0이 아래와 같은 위치에 있을 때 시계방향으로 탐색하다가 만나는 최초의 가상 노드 s1_1이며 즉, 서버 1이다.

<img width="473" alt="image" src="https://user-images.githubusercontent.com/31172248/182132377-322950c9-b7e5-4c07-813a-a88e76ab418d.png">


가상 노드의 개수를 늘리면 키의 분포가 점점 더 균등해질 것이다. (표준 편차가 작아지니까.)
* 표준편차 : 데이터가 어떻게 퍼져 나갔는지를 보이는 척도

100~200개의 가상 노드를 사용했을 경우 표준 편차 값은 평균의 5%(가상 노드가 200개인 경우)에서 10%(가상 노드가 100개인 경우) 사이다. 가상 노드의 개수가 늘어나면 표준 편차의 값은 떨어지겠지만 그만큼 가상 노드 데이터 저장공간은 많이 필요하게 된다.
그래서 이 이슈에 맞는 tradeoff 가 필요하다.

## 재배치할 키 결정
서버가 추가되거나 제거되면 데이터 일부는 재배치 해야한다. 
만약 아래 그림처럼 `서버 4`가 추가되었을 때, 서버 3과 서버4 사이의 키들이 서버4로 재배치 되어야할 것이다. 

<img width="473" alt="image" src="https://user-images.githubusercontent.com/31172248/182132420-04f87fd3-6847-41c4-a719-c8541b01eaf8.png">
