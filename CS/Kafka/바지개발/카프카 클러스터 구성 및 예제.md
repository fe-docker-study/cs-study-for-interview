# 카프카 클러스터 구성



## docker-compose를 사용해서 클러스터 구축

### 환경세팅

#### virtualBox설치

#### ubuntu설치

#### java 설치

```bash
$ sudo apt install openjdk-8-jre-headless
$ java -version
```

#### docker 설치

[Install Docker Engine on Ubuntu](https://docs.docker.com/engine/install/ubuntu/)

#### docker-compose 설치

- Docker Compose 1.27.2

```bash
$ sudo curl -L "<https://github.com/docker/compose/releases/download/1.27.2/docker-compose-$>(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
```

- 바이너리에 실행 권한 추가

```bash
$ sudo chmod +x /usr/local/bin/docker-compose
```

- Docker Compose 설치 확인

```bash
$ docker-compose -v
```



### kakfa 실행환경 세팅

#### docker-compose.yaml

- docker-compose.yaml 작성

```bash
version: '2.1'

services:
  zoo1:
    image: confluentinc/cp-zookeeper:7.1.1 #링크드인에서 카프카를 개발한 제이크렙스가 설립한 회사의 이미지
    hostname: zoo1
    container_name: zoo1
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181 #클라이언트가 주키퍼 접속을 위한 포트
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_SERVERS: zoo1:2888:3888 #2888은 동기화를 위한 포트, 3888은 클러스터 구성 시, leader를 선출하기 위한 포트
      ZOOKEEPER_TIME_TICK: 2000 # 주키퍼에서 시간 초과를 체크하는 tick 단위(밀리초)

  kafka1:
    image: confluentinc/cp-kafka:7.1.1
    hostname: kafka1
    container_name: kafka1
    ports:
      - "9092:9092"
    environment:
      KAFKA_ADVERTISED_LISTENERS: LISTENER_DOCKER_INTERNAL://kafka1:19092,LISTENER_DOCKER_EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092 #카프카 브로커를 가리키는 사용가능 주소목록. 카프카 브로커는 초기 연결시 이를 클라이언트에게 보냄
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT # the listener name is not a security protocol, listener.security.protocol.map must also be set. PLAINTEXT는 리스너가 암호화되지 않은 것을 말함.
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_DOCKER_INTERNAL #브로커 간 통신에 사용할 리스너를 정의. KAFKA_ADVERTISED_LISTENERS 가 여러개인 경우 꼭 사용해야함
      KAFKA_ZOOKEEPER_CONNECT: "zoo1:2181" #클러스터 구성에 사용할 호스트 정보. docker 네임스페이스가 호스트로 인식
      KAFKA_BROKER_ID: 1 #클러스터 각 노드에 할당할 브로커 고유 아이디
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
    depends_on: #의존하는 컨테이너 명
      - zoo1

  kafka2:
    image: confluentinc/cp-kafka:7.1.1
    hostname: kafka2
    container_name: kafka2
    ports:
      - "9093:9093"
    environment:
      KAFKA_ADVERTISED_LISTENERS: LISTENER_DOCKER_INTERNAL://kafka2:19093,LISTENER_DOCKER_EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_DOCKER_INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: "zoo1:2181"
      KAFKA_BROKER_ID: 2
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
    depends_on:
      - zoo1

  kafka3:
    image: confluentinc/cp-kafka:7.1.1
    hostname: kafka3
    container_name: kafka3
    ports:
      - "9094:9094"
    environment:
      KAFKA_ADVERTISED_LISTENERS: LISTENER_DOCKER_INTERNAL://kafka3:19094,LISTENER_DOCKER_EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_DOCKER_INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: "zoo1:2181"
      KAFKA_BROKER_ID: 3
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
    depends_on:
      - zoo1
```

- docker-compose.yaml 실행

```bash
$ sudo docker-compose -f docker-compose.yaml up -d
```

- docker-compose 종료

  ```bash
  $ sudo docker-compose -f docker-compose.yaml down
  ```

- exec - execute a command

  ```bash
  $ sudo docker exec -ti kafka1 /bin/bash
  ```

- 설치확인

```bash
$ sudo docker ps 
CONTAINER ID   IMAGE                             COMMAND                  CREATED        STATUS        PORTS                                                           NAMES
12612ae182ff   confluentinc/cp-kafka:7.1.1       "/etc/confluent/dock…"   18 hours ago   Up 18 hours   9092/tcp, 0.0.0.0:9093->9093/tcp, :::9093->9093/tcp             kafka2
85db6561f5f7   confluentinc/cp-kafka:7.1.1       "/etc/confluent/dock…"   18 hours ago   Up 18 hours   9092/tcp, 0.0.0.0:9094->9094/tcp, :::9094->9094/tcp             kafka3
d599841ede19   confluentinc/cp-kafka:7.1.1       "/etc/confluent/dock…"   18 hours ago   Up 18 hours   0.0.0.0:9092->9092/tcp, :::9092->9092/tcp                       kafka1
babf5a214c20   confluentinc/cp-zookeeper:7.1.1   "/etc/confluent/dock…"   18 hours ago   Up 18 hours   2888/tcp, 0.0.0.0:2181->2181/tcp, :::2181->2181/tcp, 3888/tcp   zoo1
```

#### confluent 설치

- 사용할 command를 제공할 confluent설치

```bash
$ curl -O <http://packages.confluent.io/archive/7.0/confluent-7.0.1.tar.gz>
$ tar -zxvf confluent-7.0.1.tar.gz
```



### kafka 테스트

```bash
$ cd confluent-7.0.1/bin
```

#### topic 생성

- topic 생성

```bash
$ ./kafka-topics --bootstrap-server localhost:9092 --create --topic kafka-test --partitions 1 --replication-factor 3
```

- topic list

```bash
$ ./kafka-topics --list --bootstrap-server localhost:9092
```

- topic 상세 확인

```bash
$ ./kafka-topics --describe --bootstrap-server localhost:9092 --topic kafka-test
```

#### producer 메시지 발행

- 메시지를 보낼 수 있는 shell 실행

```bash
$ ./kafka-console-producer --bootstrap-server localhost:9092 --topic kafka-test
>test~
>check
```

#### consumer 메시지 확인

- producer가 발행한 메시지 확인(메시지를 보내기 전에 켜둬야함)

```bash
$ ./kafka-console-consumer --bootstrap-server localhost:9092 --topic kafka-test
test~
check
```

- 이제까지 발행한 메시지 확인

```bash
$ ./kafka-console-consumer --bootstrap-server localhost:9092 --topic kafka-test --from-beginning
hello kafka-test topic test MESSAGE~!!
test message~
check!
exit

send
test
test~
check
```





## kafka Spring 연동

- MSA 프로젝트에 붙인 예제
- maven 사용
- 상품 주문이 발생할 시 product-order 서비스(producer)에서 kafka를 통해 주문 내역을 메시지로 발행
- product-catalog 서비스(consumer)에서는 kafka를 통해 비동기적으로 주문내역을 받고 DB에 반영



### Producer 예제

#### pom.xml

```xml
<!-- Kafka -->
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
</dependency>
```

#### KafkaProducerConfig.java

```java
package com.example.productorder.messagequeue;

import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.common.serialization.StringSerializer;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.annotation.EnableKafka;
import org.springframework.kafka.core.DefaultKafkaProducerFactory;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.core.ProducerFactory;

import java.util.HashMap;
import java.util.Map;

@EnableKafka
@Configuration
public class KafkaProducerConfig {

    private final static String BOOTSTRAP_SERVERS = "127.0.0.1:9092"; //전송하고자 하는 카프카 클러스터 서버의 host와 IP

    //접속할 수 있는 Kafka 정보가 들어 있는 빈
    @Bean
    public ProducerFactory<String, String> producerFactory() {
        Map<String, Object> properties = new HashMap<>();
        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);

        // 메시지 키, 메시지 값을 직렬화 하기 위한 직렬화 클래스 선언. String 객체를 전송하기 위해 String을 직렬화하는 클래스인 카프카 라이브러리 StringSerializer를 사용
        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);

        //properties를 생성 파라미터로 추가해 인스턴스를 생성. 이 인스턴스는 ProducerRecord를 전송할 떄 사용
        return new DefaultKafkaProducerFactory<>(properties);
    }

    //데이터를 전달하는 용도의 인스턴스
    @Bean
    public KafkaTemplate<String, String> kafkaTemplate() {
        return new KafkaTemplate<>(producerFactory());
    }
}
```

#### KafkaProducer.java

```java
package com.example.productorder.messagequeue;

import com.example.productorder.vo.RequestProductOrder;
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;

@Service
@Slf4j
public class KafkaProducer {
    private KafkaTemplate<String, String> kafkaTemplate;

    @Autowired
    public KafkaProducer(KafkaTemplate<String, String> kafkaTemplate) {
        this.kafkaTemplate = kafkaTemplate;
    }

    public RequestProductOrder send(String topic, RequestProductOrder requestProductOrder) {
        ObjectMapper mapper = new ObjectMapper();
        String jsonInString = "";
        try {
            //requestProductOrder 인스턴스 값은 json 포맷으로 변경
            jsonInString = mapper.writeValueAsString(requestProductOrder);
        } catch(JsonProcessingException ex) {
            ex.printStackTrace();
        }

        //send메소드를 호출하면 ProduceRecord를 생성해 프로듀서 내부에 가지고 있다가 배치 형태로 묶어서 브로커에 전송
        kafkaTemplate.send(topic, jsonInString);
        log.info("Kafka Producer sent data from product-order microservice: " + requestProductOrder);

        return requestProductOrder;
    }
}
```

#### ProductOrderController.java

- kafkaProducer.send() 메소드를 호출

```java
package com.example.productorder.controller;

import com.example.productorder.dto.ProductOrderDto;
import com.example.productorder.messagequeue.KafkaProducer;
import com.example.productorder.service.ICatalogService;
import com.example.productorder.service.IUserService;
import com.example.productorder.service.ProductOrderServiceImpl;
import com.example.productorder.vo.RequestProductOrder;
import com.example.productorder.vo.ResponseProductOrder;
import com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand;
import lombok.extern.slf4j.Slf4j;
import org.modelmapper.ModelMapper;
import org.modelmapper.convention.MatchingStrategies;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.RequestBody;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestMethod;
import org.springframework.web.bind.annotation.RestController;

@Slf4j
@RestController
public class ProductOrderController {

    @Autowired
    private ProductOrderServiceImpl productOrderServiceImpl;

    @Autowired
    private KafkaProducer kafkaProducer;

    @Autowired
    private IUserService iUserService;

    @Autowired
    private ICatalogService iCatalogService;

    @HystrixCommand
    @RequestMapping(value = "/product-order/orderProduct", method = RequestMethod.POST)
    public ResponseEntity<ResponseProductOrder> orderProduct(@RequestBody RequestProductOrder requestProductOrder) {

        //멤버인지 확인
        if(iUserService.checkUser(requestProductOrder.getUserName())) {
            log.info("{}은 가입되어 있는 회원", requestProductOrder.getUserName());
        }

        //주문할 수 있는 상품인지 확인
        if(!iCatalogService.checkProductQuantity(requestProductOrder.getProductName(), requestProductOrder.getCount())) {
            log.info("{}은 주문할 수 없는 상품", requestProductOrder.getProductName());
            return null;
        }

        ModelMapper mapper = new ModelMapper();
        mapper.getConfiguration().setMatchingStrategy(MatchingStrategies.STRICT);
        ProductOrderDto orderDto = mapper.map(requestProductOrder, ProductOrderDto.class);

        //상품 주문
        productOrderServiceImpl.orderProduct(orderDto);

        //kafka
        kafkaProducer.send("kafka-test", requestProductOrder);

        ResponseProductOrder responseProductOrder = new ResponseProductOrder();
        responseProductOrder.setProductName(requestProductOrder.getProductName());
        ResponseProductOrder responseOrder = mapper.map(orderDto, ResponseProductOrder.class);

        return new ResponseEntity<ResponseProductOrder>(responseOrder, HttpStatus.OK);
    }
}
```



### Cunsumer 예제

#### pom.xml

```xml
<!-- Kafka -->
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
</dependency>
```

#### KafkaConsumerConfig.java

```java
package com.example.productcatalog.messagequeue;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.annotation.EnableKafka;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.DefaultKafkaConsumerFactory;

import java.util.HashMap;
import java.util.Map;

@EnableKafka
@Configuration
public class KafkaConsumerConfig {

    private final static String BOOTSTRAP_SERVERS = "127.0.0.1:9092"; //데이터를 가져올 카프카 클러스터 서버의 host와 IP
    private final static String GROUP_ID = "test_group"; //컨슈머 그룹 이름. 컨슈머 그룹을 선언하지 않으면 어떤 그룹에도 속하지 않는 컨슈머로 동작

    //kafka 접속 정보가 들어 있는 빈
    @Bean
    public ConsumerFactory<String, String> consumerFactory() {
        Map<String, Object> properties = new HashMap<>();
        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);
        properties.put(ConsumerConfig.GROUP_ID_CONFIG, GROUP_ID);
        //프로듀서가 직렬화해서 전송한 데이터를 역직렬화하기 위해 역직렬화 클래스를 지정
        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);

        //properties로 지정한 카프카 컨슈머 옵션을 파라미터로 받아 인스턴스 생성.
        return new DefaultKafkaConsumerFactory<>(properties);
    }

    //접속 정보를 이용하는 실제 Listener
    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, String> kafkaListenerContainerFactory(){
        //등록한 설정 정보를 등록
        ConcurrentKafkaListenerContainerFactory<String, String> kafkaListenerContainerFactory = new ConcurrentKafkaListenerContainerFactory<>();
        kafkaListenerContainerFactory.setConsumerFactory(consumerFactory());

        return kafkaListenerContainerFactory;
    }
}
```

#### KafkaConsumer.java

```java
package com.example.productcatalog.messagequeue;

import com.example.productcatalog.jpa.IProductCatalogRepository;
import com.example.productcatalog.jpa.ProductCatalogEntity;
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;

import java.util.HashMap;
import java.util.Map;

@Service
@Slf4j
public class KafkaConsumer {
    IProductCatalogRepository repository;

    @Autowired
    public KafkaConsumer(IProductCatalogRepository repository) {
        this.repository = repository;
    }

    @KafkaListener(topics = "kafka-test")
    public void update(String kafkaMessage){
        log.info("Kafka Message: -> " + kafkaMessage);

        Map<Object, Object> map = new HashMap<>();
        ObjectMapper mapper = new ObjectMapper();
        try{
            //stirng으로 들어오는 message를 json 타입으로 변환하는 코드
            map = mapper.readValue(kafkaMessage, new TypeReference<Map<Object, Object>>() {});
        }catch (JsonProcessingException ex){
            ex.printStackTrace();
        }

        ProductCatalogEntity productCatalogEntity = repository.findByProductName((String)map.get("productName"));
        if(productCatalogEntity!=null){
            productCatalogEntity.setQuantity(productCatalogEntity.getQuantity() - (Integer)map.get("count"));
            repository.save(productCatalogEntity);
        }
    }
}
```